\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\@writefile{toc}{\contentsline {section}{\numberline {1}Question 1}{1}{section.1}\protected@file@percent }
\newlabel{eq:Q1_distribution}{{1}{1}{Question 1}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The distribution in Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Q1_distribution}\unskip \@@italiccorr )}} visualised over $x \in [-50, 50]$.\relax }}{1}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Q1_distribution}{{1}{1}{The distribution in Equation \eqref {eq:Q1_distribution} visualised over $x \in [-50, 50]$.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The efficiency of the rejection sampling procedure for 1000 samples under different parametrisations of the proposal distribution $q(x)$. Note that as the scale parameter $k$ was determined automatically, the starting points for each of the different distributions is not consistent.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:Q1_rejection_sampling_dists}{{2}{3}{The efficiency of the rejection sampling procedure for 1000 samples under different parametrisations of the proposal distribution $q(x)$. Note that as the scale parameter $k$ was determined automatically, the starting points for each of the different distributions is not consistent.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The samples from $p(x)$ using rejection sampling and a proposal distribution $\text  {st}_{2}(x \vert 3, 5)$. \relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:Q1_rejection_sampling_samples}{{3}{3}{The samples from $p(x)$ using rejection sampling and a proposal distribution $\text {st}_{2}(x \vert 3, 5)$. \relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The estimates of interest using the samples of $p(x)$ obtained using rejection sampling.\relax }}{4}{table.caption.5}\protected@file@percent }
\newlabel{tab:Q1_estimates}{{1}{4}{The estimates of interest using the samples of $p(x)$ obtained using rejection sampling.\relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The effect of the number of samples on the estimates. (a) shows the variation in the estimate and (b) shows the estimate error as a function of the number of samples. Note that the estimates were obtained as a once-off result, and as such this figure does not effectively convey the variation in the estimates for a given number of samples. However, it is clear that increasing the number of samples reduces the error in the estimates.\relax }}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Q1_estimate_error}{{4}{4}{The effect of the number of samples on the estimates. (a) shows the variation in the estimate and (b) shows the estimate error as a function of the number of samples. Note that the estimates were obtained as a once-off result, and as such this figure does not effectively convey the variation in the estimates for a given number of samples. However, it is clear that increasing the number of samples reduces the error in the estimates.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Question 2}{5}{section.2}\protected@file@percent }
\newlabel{eq:unnormalised_posterior}{{10}{6}{Question 2}{equation.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The prior, likelihood function and the unnormalised posterior for the second problem.\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:Q2_distributions}{{5}{6}{The prior, likelihood function and the unnormalised posterior for the second problem.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The acceptance ratio and the estimates of interest for different proposal distributions with different initial parameters. Note that the estimates do not effectively capture the variation in the value of the estimates, and hence are biased as they were determined once off. A tuning period of $50\%$, a thinning size of 10, and 100 000 iterations were performed for all experiments.\relax }}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:Q2_results}{{2}{7}{The acceptance ratio and the estimates of interest for different proposal distributions with different initial parameters. Note that the estimates do not effectively capture the variation in the value of the estimates, and hence are biased as they were determined once off. A tuning period of $50\%$, a thinning size of 10, and 100 000 iterations were performed for all experiments.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The effect of the number of samples on the MCMC-based estimates. (a) shows the variation in the estimate and (b) shows the estimate error as a function of the number of samples. Note that the estimates were obtained as a once-off result, and as such this figure does not effectively convey the variation in the estimates for a given number of samples. However, it is clear that increasing the number of samples reduces the error in the estimates. A Gaussian distribution with a mean of 0 and a variance of $2^2$ was used for all iterations.\relax }}{7}{figure.caption.9}\protected@file@percent }
\newlabel{fig:Q2_estimate_error}{{6}{7}{The effect of the number of samples on the MCMC-based estimates. (a) shows the variation in the estimate and (b) shows the estimate error as a function of the number of samples. Note that the estimates were obtained as a once-off result, and as such this figure does not effectively convey the variation in the estimates for a given number of samples. However, it is clear that increasing the number of samples reduces the error in the estimates. A Gaussian distribution with a mean of 0 and a variance of $2^2$ was used for all iterations.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Question 3}{8}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The observed material data for the third problem.\relax }}{8}{figure.caption.10}\protected@file@percent }
\newlabel{fig:Q3_observed_data}{{7}{8}{The observed material data for the third problem.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}PyMC3 analysis}{8}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Application and inference}{9}{subsection.3.2}\protected@file@percent }
\newlabel{eq:predefined_posterior}{{17}{9}{Application and inference}{equation.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Samples from the prior and posterior distributions over $\{\tilde  {\bm  {\theta }}, \eta \}$.\relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:Q3_pymc3_prior_post_samples}{{8}{10}{Samples from the prior and posterior distributions over $\{\tilde {\boldsymbol \theta }, \eta \}$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Samples from the prior and posterior predictive distributions for the linear elasticity-linear hardening material model using the prior and posterior distribution \texttt  {PyMC3} samples.\relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:Q3_pymc3_prior_post_predictive_samples}{{9}{11}{Samples from the prior and posterior predictive distributions for the linear elasticity-linear hardening material model using the prior and posterior distribution \texttt {PyMC3} samples.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Samples from the prior and posterior predictive distributions for the linear elasticity-linear hardening material model obtained using \texttt  {PyMC3}.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:Q3_pymc3_prior_post_predictive_PyMC3}{{10}{11}{Samples from the prior and posterior predictive distributions for the linear elasticity-linear hardening material model obtained using \texttt {PyMC3}.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The effect of the prior hyper-parameters on the MAP estimates of the model hyper-parameters.\relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig:Q3_hyperparameter_effects}{{11}{11}{The effect of the prior hyper-parameters on the MAP estimates of the model hyper-parameters.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The likelihood and log-likelihood function for $E = 200 GPa, \sigma _{y_0} = 300 MPa, H = 45 GPa$ and an unknown noise.\relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig:Q3_noise_LL}{{12}{12}{The likelihood and log-likelihood function for $E = 200 GPa, \sigma _{y_0} = 300 MPa, H = 45 GPa$ and an unknown noise.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Samples from the marginal distribution $ p(\sigma _{y_0} \vert \bm  {\sigma }_{meas}, \bm  {\epsilon })$ over the yield stress.\relax }}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:Q3_yield}{{13}{13}{Samples from the marginal distribution $ p(\sigma _{y_0} \vert \boldsymbol \sigma _{meas}, \boldsymbol \epsilon )$ over the yield stress.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The empirical posterior predictive distribution where $\epsilon = 0.001$ for three different variations of the posterior predictive distribution.\relax }}{14}{figure.caption.17}\protected@file@percent }
\newlabel{fig:Q3_post_predict_samples}{{14}{14}{The empirical posterior predictive distribution where $\epsilon = 0.001$ for three different variations of the posterior predictive distribution.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Question 4}{15}{section.4}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{D41D8CD98F00B204E9800998ECF8427E}
\gdef \@abspage@last{15}
