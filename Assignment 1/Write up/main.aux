\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\@writefile{toc}{\contentsline {section}{\numberline {1}Question 1}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Part A: Use the model}{1}{subsection.1.1}\protected@file@percent }
\newlabel{eq:Model1}{{1}{1}{Part A: Use the model}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}The model distribution}{1}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The analytical and empirical distributions of $p(x) = \mathcal  {N}(6, 1.5^2)$, where the empirical distribution is demonstrated in a), b) and c) as a function of the number of samples.\relax }}{1}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Q1a_1}{{1}{1}{The analytical and empirical distributions of $p(x) = \mathcal {N}(6, 1.5^2)$, where the empirical distribution is demonstrated in a), b) and c) as a function of the number of samples.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The CDF of $p(x)$}{1}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The CDF of $p(x)$.\relax }}{2}{figure.caption.3}\protected@file@percent }
\newlabel{fig:Q1a_2}{{2}{2}{The CDF of $p(x)$.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Analytical probabilities versus Monte Carlo estimates}{2}{subsection.1.3}\protected@file@percent }
\newlabel{eq:p_4_x_5}{{3}{2}{Analytical probabilities versus Monte Carlo estimates}{equation.1.3}{}}
\newlabel{eq:MonteCarlo}{{5}{2}{Analytical probabilities versus Monte Carlo estimates}{equation.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A visualisation of the indicator functions and $p(x)$, and the result of the multiplication of $f_4(x)$ and $f_5(x)$ with $p(x)$.\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:Q1a_3}{{3}{3}{A visualisation of the indicator functions and $p(x)$, and the result of the multiplication of $f_4(x)$ and $f_5(x)$ with $p(x)$.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The calculation of $P(4 \leq x \leq 5)$ by using the CDF of $p(x)$ and Monte Carlo integration.\relax }}{3}{table.caption.5}\protected@file@percent }
\newlabel{tab:probability_calculation}{{1}{3}{The calculation of $P(4 \leq x \leq 5)$ by using the CDF of $p(x)$ and Monte Carlo integration.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Percentiles}{3}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The expected value of the Monte Carlo estimate of $P_x(4 \leq x \leq 5)$ and the estimation error as a function of the number of samples. In a) we see that the variance around the estimate decreases with sample number. In b) we see an increase in the accuracy of the estimate of $P_x(4 \leq x \leq 5)$ as the number of samples increase.\relax }}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Q1a_4}{{4}{4}{The expected value of the Monte Carlo estimate of $P_x(4 \leq x \leq 5)$ and the estimation error as a function of the number of samples. In a) we see that the variance around the estimate decreases with sample number. In b) we see an increase in the accuracy of the estimate of $P_x(4 \leq x \leq 5)$ as the number of samples increase.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The CDF of $p(x)$ and the $7^{th}$ percentile thereof. Notice how the \texttt  {scipy.stats} estimate and the Monte Carlo estimate are very close.\relax }}{4}{figure.caption.8}\protected@file@percent }
\newlabel{fig:Q1a_5}{{5}{4}{The CDF of $p(x)$ and the $7^{th}$ percentile thereof. Notice how the \texttt {scipy.stats} estimate and the Monte Carlo estimate are very close.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The calculation of the $7^{th}$ percentile of $x$ by using the quantile function of $p(x)$ and Monte Carlo integration.\relax }}{4}{table.caption.7}\protected@file@percent }
\newlabel{tab:percentile_calculation}{{2}{4}{The calculation of the $7^{th}$ percentile of $x$ by using the quantile function of $p(x)$ and Monte Carlo integration.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Numerical quadrature vs Monte-Carlo integration}{4}{subsubsection.1.3.2}\protected@file@percent }
\newlabel{eq:Q1_complex_function}{{10}{4}{Numerical quadrature vs Monte-Carlo integration}{equation.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The expected value of the Monte Carlo estimate of the $7^{th}$ percentile and the estimation error as a function of the number of samples. In a) we see that the variance around the estimate decreases with sample number. In b) we see an increase in the accuracy of the estimate of $7^{th}$ percentile as the number of samples increase.\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:Q1a_6}{{6}{5}{The expected value of the Monte Carlo estimate of the $7^{th}$ percentile and the estimation error as a function of the number of samples. In a) we see that the variance around the estimate decreases with sample number. In b) we see an increase in the accuracy of the estimate of $7^{th}$ percentile as the number of samples increase.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The calculation of $\mathbb  {E}\{ \vert e^x \cdot \qopname  \relax o{cos}\left ( x^2 \right ) \vert \}$, using numerical quadrature and Monte Carlo integration.\relax }}{5}{table.caption.10}\protected@file@percent }
\newlabel{tab:complex_expectations}{{3}{5}{The calculation of $\mathbb {E}\{ \vert e^x \cdot \cos \left ( x^2 \right ) \vert \}$, using numerical quadrature and Monte Carlo integration.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The expected value of the Monte Carlo estimate of $\mathbb  {E}_{x\sim p(x)}\{\vert e^x \cdot \qopname  \relax o{cos}\left ( x^2 \right ) \vert \}$ and the estimation error as a function of the number of samples. In a) we see that the variance around the estimate decreases with sample number. In b) we see an increase in the accuracy of the estimate as the number of samples increase.\relax }}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:Q1a_7}{{7}{5}{The expected value of the Monte Carlo estimate of $\mathbb {E}_{x\sim p(x)}\{\vert e^x \cdot \cos \left ( x^2 \right ) \vert \}$ and the estimation error as a function of the number of samples. In a) we see that the variance around the estimate decreases with sample number. In b) we see an increase in the accuracy of the estimate as the number of samples increase.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Log-likelihood calculations}{5}{subsubsection.1.3.3}\protected@file@percent }
\newlabel{eq:Q1a_data}{{11}{5}{Log-likelihood calculations}{equation.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The available training data used for performing maximum likelihood estimation for the model given in Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Model1}\unskip \@@italiccorr )}}.\relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:Q1b_1}{{8}{6}{The available training data used for performing maximum likelihood estimation for the model given in Equation \eqref {eq:Model1}.\relax }{figure.caption.12}{}}
\newlabel{eq:Q1_log_likelihood}{{14}{6}{Log-likelihood calculations}{equation.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Part B: Maximum likelihood estimates}{6}{subsection.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The optimal model parameters and model log-likelihood for different parameter estimation methods.\relax }}{7}{table.caption.13}\protected@file@percent }
\newlabel{tab:Q1b_table}{{4}{7}{The optimal model parameters and model log-likelihood for different parameter estimation methods.\relax }{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The likelihood function over the model parameters $\mu $ and $\sigma $ for the methods of interest. In a) the optimal parameters are superimposed over the likelihood function, and in b) the optimal models are superimposed over the data samples.\relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig:Q1b_2}{{9}{8}{The likelihood function over the model parameters $\mu $ and $\sigma $ for the methods of interest. In a) the optimal parameters are superimposed over the likelihood function, and in b) the optimal models are superimposed over the data samples.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Part C: Sampling distributions}{8}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Empirical and analytical sampling distributions}{8}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The 95$\%$ confidence interval on $\mu $ for a model where the variance is also a function of the data.\relax }}{8}{figure.caption.15}\protected@file@percent }
\newlabel{fig:Q1b_3}{{10}{8}{The 95$\%$ confidence interval on $\mu $ for a model where the variance is also a function of the data.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The empirical and analytical sampling distributions for the model presented in Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Model1}\unskip \@@italiccorr )}} with $\mu =6$ and $\sigma =1.5$. The empirical distribution was generated by drawing 20 samples from the true model $x\sim \mathbb  {N}(6, 1.5^2)$ and then calculating the maximum likelihood estimate for $\hat  {\mu }$ repeatedly.\relax }}{9}{figure.caption.16}\protected@file@percent }
\newlabel{fig:Q1c_1}{{11}{9}{The empirical and analytical sampling distributions for the model presented in Equation \eqref {eq:Model1} with $\mu =6$ and $\sigma =1.5$. The empirical distribution was generated by drawing 20 samples from the true model $x\sim \mathbb {N}(6, 1.5^2)$ and then calculating the maximum likelihood estimate for $\hat {\mu }$ repeatedly.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Estimator bias}{9}{subsubsection.1.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The bias of to estimators for the variance as a function of the number of expectation iterations. Notice that a) has a bias offset, while b) does not.\relax }}{10}{figure.caption.17}\protected@file@percent }
\newlabel{fig:Q1c_2}{{12}{10}{The bias of to estimators for the variance as a function of the number of expectation iterations. Notice that a) has a bias offset, while b) does not.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Question 2}{11}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem formulation}{11}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Parameters of the four second order polynomial models.\relax }}{11}{table.caption.18}\protected@file@percent }
\newlabel{tab:Q2_table1}{{5}{11}{Parameters of the four second order polynomial models.\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The four second order polynomials superimposed on the data samples and the absolute sample error. In a) the four models are visualised and in b) the absolute error between the available data samples and the various model function values $f(x_n)$ is shown.\relax }}{11}{figure.caption.19}\protected@file@percent }
\newlabel{fig:Q2a_1}{{13}{11}{The four second order polynomials superimposed on the data samples and the absolute sample error. In a) the four models are visualised and in b) the absolute error between the available data samples and the various model function values $f(x_n)$ is shown.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Formulating the maximum likelihood estimation problem}{11}{subsection.2.2}\protected@file@percent }
\abx@aux@cite{0}{Petersen2006TheMC}
\abx@aux@segm{0}{0}{Petersen2006TheMC}
\abx@aux@cite{0}{Petersen2006TheMC}
\abx@aux@segm{0}{0}{Petersen2006TheMC}
\newlabel{eq:Q2_log_likelihood_function}{{32}{12}{Formulating the maximum likelihood estimation problem}{equation.2.32}{}}
\newlabel{eq:partial_ML_regression}{{33}{12}{Formulating the maximum likelihood estimation problem}{equation.2.33}{}}
\abx@aux@cite{0}{bishop2006}
\abx@aux@segm{0}{0}{bishop2006}
\newlabel{eq:Phi_matrix}{{35}{13}{Formulating the maximum likelihood estimation problem}{equation.2.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Likelihood function intuition}{14}{subsection.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces model Log-likelihood for the observed data for the four second order polynomial models.\relax }}{14}{table.caption.20}\protected@file@percent }
\newlabel{tab:Q2_table2}{{6}{14}{model Log-likelihood for the observed data for the four second order polynomial models.\relax }{table.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The different models with parameters from Table \ref  {tab:Q2_table3} visualised over the available data samples.\relax }}{15}{figure.caption.21}\protected@file@percent }
\newlabel{fig:Q2a_2}{{14}{15}{The different models with parameters from Table \ref {tab:Q2_table3} visualised over the available data samples.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Linear regression maximum likelihood estimation}{15}{subsection.2.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The model parameters for the maximum likelihood estimate for different sets of unknown model parameters. The order was increased linearly from $k=0$ to $k=9$.\relax }}{15}{table.caption.22}\protected@file@percent }
\newlabel{tab:Q2_table3}{{7}{15}{The model parameters for the maximum likelihood estimate for different sets of unknown model parameters. The order was increased linearly from $k=0$ to $k=9$.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Parameter confidence interval}{15}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The confidence interval around $\omega _0$ as a function of the model order. Notice how after $K=2$ the confidence interval appears to smooth out.\relax }}{16}{figure.caption.23}\protected@file@percent }
\newlabel{fig:Q2a_3}{{15}{16}{The confidence interval around $\omega _0$ as a function of the model order. Notice how after $K=2$ the confidence interval appears to smooth out.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Model selection investigation}{16}{subsection.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The polynomial function of interest for the model selection investigation.\relax }}{17}{figure.caption.24}\protected@file@percent }
\newlabel{fig:Q2b_1}{{16}{17}{The polynomial function of interest for the model selection investigation.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The results from the model selection investigation for four selection methods as a function of the model polynomial order. In a) 1000 samples were drawn from $f(x)$ and the model variance was found using maximum likelihood estimation. In b) 1000 samples were used and the model variance was set to $\sigma ^2=0.1^2$. In c) 100 samples were used and the model variance was found using maximum likelihood estimation. In d) 100 samples were used and the model variance was set to $\sigma ^2=0.1^2$.\relax }}{18}{figure.caption.25}\protected@file@percent }
\newlabel{fig:Q2b_2}{{17}{18}{The results from the model selection investigation for four selection methods as a function of the model polynomial order. In a) 1000 samples were drawn from $f(x)$ and the model variance was found using maximum likelihood estimation. In b) 1000 samples were used and the model variance was set to $\sigma ^2=0.1^2$. In c) 100 samples were used and the model variance was found using maximum likelihood estimation. In d) 100 samples were used and the model variance was set to $\sigma ^2=0.1^2$.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Bias-variance investigation}{18}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The model bias and variance for the optimal polynomial order from the different model selection methods using model validation on all the training data and by performing k-fold cross validation. Note that in this example the model variance was a free parameter, and the AIC and BIC metrics selected the optimal order as zero. Hence the bias resembles the zero-centered data and the variance is constant.\relax }}{20}{figure.caption.26}\protected@file@percent }
\newlabel{fig:Q2b_3}{{18}{20}{The model bias and variance for the optimal polynomial order from the different model selection methods using model validation on all the training data and by performing k-fold cross validation. Note that in this example the model variance was a free parameter, and the AIC and BIC metrics selected the optimal order as zero. Hence the bias resembles the zero-centered data and the variance is constant.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Question 3}{21}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model visualisation}{21}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The generated signals under different noise distributions.\relax }}{21}{figure.caption.27}\protected@file@percent }
\newlabel{fig:Q3a_1}{{19}{21}{The generated signals under different noise distributions.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Model estimation}{21}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The sampled data for the maximum likelihood estimation problem.\relax }}{22}{figure.caption.28}\protected@file@percent }
\newlabel{fig:Q3b_1}{{20}{22}{The sampled data for the maximum likelihood estimation problem.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Gaussian noise problem formulation}{22}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Laplacian noise model formulation}{23}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{eq:laplace}{{68}{23}{Laplacian noise model formulation}{equation.3.68}{}}
\newlabel{eq:log_laplace}{{69}{23}{Laplacian noise model formulation}{equation.3.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A \texttt  {Python} implementation of the Laplacian and log-Laplacian distribution. \relax }}{23}{figure.caption.29}\protected@file@percent }
\newlabel{fig:Q3b_laplace}{{21}{23}{A \texttt {Python} implementation of the Laplacian and log-Laplacian distribution. \relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Estimator comparison}{24}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The $\theta _1$ and $\theta _2$ parameters for the models with a Gaussian and Laplacian noise distribution assumption.\relax }}{25}{table.caption.30}\protected@file@percent }
\newlabel{tab:Q3b_table1}{{8}{25}{The $\theta _1$ and $\theta _2$ parameters for the models with a Gaussian and Laplacian noise distribution assumption.\relax }{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The optimal Gaussian and Laplacian models fitted to the sampled data in Figure \ref  {fig:Q3b_1}. a) shows the Gaussian model, b) shows the Laplacian model, and c) shows the sample negative log-likelihood for both models.\relax }}{25}{figure.caption.31}\protected@file@percent }
\newlabel{fig:Q3b_2}{{22}{25}{The optimal Gaussian and Laplacian models fitted to the sampled data in Figure \ref {fig:Q3b_1}. a) shows the Gaussian model, b) shows the Laplacian model, and c) shows the sample negative log-likelihood for both models.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Model bias and variance}{26}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The data generated from $x_n=f(x_n)+\epsilon _n$, where the noise is sampled from a Student-t distribution with two degrees of freedom. The sampling frequency was $F_s=100Hz$.\relax }}{26}{figure.caption.32}\protected@file@percent }
\newlabel{fig:Q3c_1}{{23}{26}{The data generated from $x_n=f(x_n)+\epsilon _n$, where the noise is sampled from a Student-t distribution with two degrees of freedom. The sampling frequency was $F_s=100Hz$.\relax }{figure.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The $\theta _1$ and $\theta _2$ parameters for the models with a Guassian and Laplacian noise distribution assumption.\relax }}{27}{table.caption.33}\protected@file@percent }
\newlabel{tab:Q3b_table2}{{9}{27}{The $\theta _1$ and $\theta _2$ parameters for the models with a Guassian and Laplacian noise distribution assumption.\relax }{table.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces The optimal Gaussian and Laplacian models fitted to the sampled data in Figure \ref  {fig:Q3c_1}. a) shows the Gaussian model, b) shows the Laplacian model, and c) shows the sample negative log-likelihood for both models.\relax }}{28}{figure.caption.34}\protected@file@percent }
\newlabel{fig:Q3c_2}{{24}{28}{The optimal Gaussian and Laplacian models fitted to the sampled data in Figure \ref {fig:Q3c_1}. a) shows the Gaussian model, b) shows the Laplacian model, and c) shows the sample negative log-likelihood for both models.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The bias, variance and expected value for $\theta _1$ as a function of the number of data samples for the Gaussian and Laplacian noise models.\relax }}{29}{figure.caption.35}\protected@file@percent }
\newlabel{fig:Q3c_3}{{25}{29}{The bias, variance and expected value for $\theta _1$ as a function of the number of data samples for the Gaussian and Laplacian noise models.\relax }{figure.caption.35}{}}
\abx@aux@cite{0}{Burden2016}
\abx@aux@segm{0}{0}{Burden2016}
\abx@aux@cite{0}{Burden2016}
\abx@aux@segm{0}{0}{Burden2016}
\@writefile{toc}{\contentsline {section}{\numberline {4}Question 4}{30}{section.4}\protected@file@percent }
\newlabel{eq:linear_system_eq}{{77}{30}{Question 4}{equation.4.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Unforced system investigation}{30}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The displacement and velocity response of the unforced system. The Euler and Runge-Kutta solutions are superimposed over one another to demonstrate that the sampling frequency $F_s$ is suitable.\relax }}{31}{figure.caption.36}\protected@file@percent }
\newlabel{fig:Q4a_1}{{26}{31}{The displacement and velocity response of the unforced system. The Euler and Runge-Kutta solutions are superimposed over one another to demonstrate that the sampling frequency $F_s$ is suitable.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}System response}{31}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}System estimation}{31}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The predicted model displacement and velocity response and the absolute error between the actual and predicted model response under two different initial values. Notice that the predicted model is able to predict the response with great precision, regardless of the initial conditions. Note that the model noise $\sigma $ was assumed to be zero.\relax }}{32}{figure.caption.37}\protected@file@percent }
\newlabel{fig:Q4a_2}{{27}{32}{The predicted model displacement and velocity response and the absolute error between the actual and predicted model response under two different initial values. Notice that the predicted model is able to predict the response with great precision, regardless of the initial conditions. Note that the model noise $\sigma $ was assumed to be zero.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}System parameter recovery}{32}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{section:unforced_parameter_recovery}{{4.1.3}{32}{System parameter recovery}{subsubsection.4.1.3}{}}
\newlabel{eq:unforced_parameter_recovery}{{91}{32}{System parameter recovery}{equation.4.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The displacement and velocity response of the forced system. The Euler and Runge-Kutta solutions are superimposed over one another to demonstrate that the sampling frequency $F_s$ is suitable.\relax }}{33}{figure.caption.38}\protected@file@percent }
\newlabel{fig:Q4b_1}{{28}{33}{The displacement and velocity response of the forced system. The Euler and Runge-Kutta solutions are superimposed over one another to demonstrate that the sampling frequency $F_s$ is suitable.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Forced system investigation}{33}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}System response}{33}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}System estimation}{33}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces The predicted model displacement and velocity response and the absolute error between the actual and predicted model response under two different initial values. Notice that the predicted model is able to predict the response with great precision, regardless of the initial conditions. Note that the model noise $\sigma $ was assumed to be zero.\relax }}{34}{figure.caption.39}\protected@file@percent }
\newlabel{fig:Q4b_2}{{29}{34}{The predicted model displacement and velocity response and the absolute error between the actual and predicted model response under two different initial values. Notice that the predicted model is able to predict the response with great precision, regardless of the initial conditions. Note that the model noise $\sigma $ was assumed to be zero.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}System parameter recovery}{34}{subsubsection.4.2.3}\protected@file@percent }
\newlabel{section:forced_parameter_recovery}{{4.2.3}{34}{System parameter recovery}{subsubsection.4.2.3}{}}
\newlabel{eq:forced_parameter_recovery}{{100}{35}{System parameter recovery}{equation.4.100}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces The recovered system parameters for the forced system.\relax }}{35}{table.caption.40}\protected@file@percent }
\newlabel{tab:Q4b_table1}{{10}{35}{The recovered system parameters for the forced system.\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Model errors}{35}{subsection.4.3}\protected@file@percent }
\newlabel{eq:time_varying_model}{{101}{35}{Model errors}{equation.4.101}{}}
\newlabel{eq:constant_model_LDS}{{102}{35}{Model errors}{equation.4.102}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{35}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces The predicted model displacement, velocity response and the absolute error between the actual and predicted model response under the original initial conditions for $\alpha = 0, 0.5$ and $1$. Notice the degradation in the model response as $\alpha $ increases. In d) the estimated model parameters as a function of $\alpha $ are shown and it is clear that the predicted parameters are greatly affected by a stiffness that is a function of time.\relax }}{36}{figure.caption.41}\protected@file@percent }
\newlabel{fig:Q4c_1}{{30}{36}{The predicted model displacement, velocity response and the absolute error between the actual and predicted model response under the original initial conditions for $\alpha = 0, 0.5$ and $1$. Notice the degradation in the model response as $\alpha $ increases. In d) the estimated model parameters as a function of $\alpha $ are shown and it is clear that the predicted parameters are greatly affected by a stiffness that is a function of time.\relax }{figure.caption.41}{}}
\abx@aux@read@bbl@mdfivesum{A33E724E308BE1AEF19D70D3BF995D2A}
\abx@aux@defaultrefcontext{0}{Petersen2006TheMC}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop2006}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Burden2016}{none/global//global/global}
\gdef \@abspage@last{37}
